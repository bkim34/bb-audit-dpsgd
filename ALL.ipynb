{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting worst-case inital model parameters through pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "from models import Models\n",
    "from utils.data import load_data\n",
    "from audit_model import test_model\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "### Pre-train on half of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1065c3110>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "data_name = 'mnist'\n",
    "lr = 0.01\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load full dataset\n",
    "X, y, out_dim = load_data(data_name, None, device=device, split='train')\n",
    "X_test, y_test, _ = load_data(data_name, None, device=device, split='test')\n",
    "\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use only first half of dataset for pre-training\n",
    "X_train, y_train = X[:len(X)//2], y[:len(X)//2]\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Models['cnn'](X_train.shape, out_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:48<00:00,  9.68s/it, loss=0.265] \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "pbar = tqdm(range(n_epochs))\n",
    "losses = []\n",
    "save_model_epochs = [1, 2, 3, 4]\n",
    "saved_models = []\n",
    "for curr_epoch in pbar:\n",
    "    for curr_X, curr_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(curr_X)\n",
    "        loss = criterion(output, curr_y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        pbar.set_postfix({'loss': losses[-1]})\n",
    "    \n",
    "    if curr_epoch in save_model_epochs:\n",
    "        saved_models.append(deepcopy(model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bwm5pv5j0ms2gv7c21gj70k00000gn/T/ipykernel_97016/2421651748.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('pretrained_models/cnn_mnist_half.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('pretrained_models/cnn_mnist_half.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (%): 92.990\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_acc = test_model(model, X_test, y_test) * 100\n",
    "print(f'Test accuracy (%): {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make both directories if missing\n",
    "os.makedirs('pretrained_models', exist_ok=True)\n",
    "os.makedirs('pretrained_models/cnn_mnist_half_epochs', exist_ok=True)\n",
    "\n",
    "torch.save(model.cpu().state_dict(), 'pretrained_models/cnn_mnist_half.pt')\n",
    "for epoch, m in zip(save_model_epochs, saved_models):\n",
    "    torch.save(m.cpu().state_dict(),\n",
    "               f'pretrained_models/cnn_mnist_half_epochs/{epoch}epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.cpu().state_dict(), f'pretrained_models/cnn_mnist_half.pt')\n",
    "for i, (save_model_epoch, model) in enumerate(zip(save_model_epochs, saved_models)):\n",
    "    torch.save(model.cpu().state_dict(), f'pretrained_models/cnn_mnist_half_epochs/{save_model_epoch}epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save remaining half to ensure no overlap\n",
    "folder = f'data/{data_name}_finetune_half/'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "X_finetune, y_finetune = X[len(X)//2:], y[len(y)//2:]\n",
    "\n",
    "np.save(f'{folder}/X_train.npy', X_finetune.cpu().numpy())\n",
    "np.save(f'{folder}/y_train.npy', y_finetune.cpu().numpy())\n",
    "np.save(f'{folder}/X_test.npy', X_test.cpu().numpy())\n",
    "np.save(f'{folder}/y_test.npy', y_test.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IGNORE ALL CIFAR 10 for NOW Deleted for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##CONTINUE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Auditing DP-SGD in black-box setting\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from opacus.accountants.utils import get_noise_multiplier\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import dill\n",
    "\n",
    "from models import Models\n",
    "from utils.data import load_data\n",
    "from utils.dpsgd import clip_and_accum_grads\n",
    "from utils.audit import compute_eps_lower_from_mia\n",
    "from utils.clipbkd import craft_clipbkd, choose_worstcase_label\n",
    "\n",
    "def xavier_init_model(model):\n",
    "    \"\"\"Initialize model using Xavier initialization\"\"\"\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    model.apply(init_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, X, y, epsilon, delta, max_grad_norm, n_epochs, lr, device='cpu', init_model=None, block_size=1024, out_dim=10):\n",
    "    \"\"\"Train model w/ DP-SGD (no sub-sampling + gradients are summed instead of averaged)\"\"\"\n",
    "    # initialize model, loss function, and optimizer\n",
    "    if init_model is None:\n",
    "        model = Models[model_name](X.shape, out_dim=out_dim).to(device)\n",
    "        xavier_init_model(model)\n",
    "    else:\n",
    "        model = copy.deepcopy(init_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    # set noise level\n",
    "    if epsilon is not None:\n",
    "        # no subsampling, i.e., sample rate = 1\n",
    "        noise_multiplier = get_noise_multiplier(target_epsilon=epsilon, target_delta=delta, sample_rate=1,\n",
    "            epochs=n_epochs, accountant='prv')\n",
    "    else:\n",
    "        noise_multiplier = 0\n",
    "    \n",
    "    # train model for n_epochs\n",
    "    grad_norms = []\n",
    "    for epoch in tqdm(range(n_epochs), leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        accum_grad, curr_grad_norms = clip_and_accum_grads(model, X, y, optimizer, criterion, max_grad_norm, block_size=block_size)\n",
    "        if epoch == 0:\n",
    "            # save per-sample gradient norms from first epoch\n",
    "            grad_norms.append(curr_grad_norms)\n",
    "\n",
    "        # accumulate per-sample gradients and add noise\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                curr_grad = accum_grad[name]\n",
    "\n",
    "                if noise_multiplier > 0 and max_grad_norm is not None:\n",
    "                    # add noise\n",
    "                    curr_grad = curr_grad + noise_multiplier * max_grad_norm * torch.randn_like(curr_grad)\n",
    "                \n",
    "                # update gradient of parameter\n",
    "                param.grad = curr_grad\n",
    "        \n",
    "        # update parameter\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, grad_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, X, y, batch_size=128):\n",
    "    \"\"\"Test trained model on test set\"\"\"\n",
    "    test_loader = DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for curr_X, curr_y in test_loader:\n",
    "            curr_y_hat = torch.argmax(model(curr_X), dim=1)\n",
    "            acc += torch.sum(curr_y_hat == curr_y).cpu().item()\n",
    "    model.train()\n",
    "    \n",
    "    return acc / len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(out_folder, outputs, losses, all_grad_norms, train_set_accs, test_set_accs, fit_world_only, save_grad_norms):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    # create folder if not exists\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    # save random state\n",
    "    random_state = {\n",
    "        'np': np.random.get_state(),\n",
    "        'torch': torch.random.get_rng_state()\n",
    "    }\n",
    "    dill.dump(random_state, open(f'{out_folder}/random_state.dill', 'wb'))\n",
    "\n",
    "    # save intermediate values\n",
    "    if fit_world_only:\n",
    "        np.save(f'{out_folder}/outputs_{fit_world_only}.npy', outputs[fit_world_only])\n",
    "        np.save(f'{out_folder}/losses_{fit_world_only}.npy', losses[fit_world_only])\n",
    "        if save_grad_norms:\n",
    "            np.save(f'{out_folder}/all_grad_norms_{fit_world_only}.npy', all_grad_norms[fit_world_only])\n",
    "\n",
    "        if fit_world_only == 'out':\n",
    "            np.save(f'{out_folder}/train_set_accs.npy', train_set_accs)\n",
    "            np.save(f'{out_folder}/test_set_accs.npy', test_set_accs)\n",
    "    else:\n",
    "        np.save(f'{out_folder}/outputs_in.npy', outputs['in'])\n",
    "        np.save(f'{out_folder}/outputs_out.npy', outputs['out'])\n",
    "        np.save(f'{out_folder}/train_set_accs.npy', train_set_accs)\n",
    "        np.save(f'{out_folder}/test_set_accs.npy', test_set_accs)\n",
    "        np.save(f'{out_folder}/losses_in.npy', losses['in'])\n",
    "        np.save(f'{out_folder}/losses_out.npy', losses['out'])\n",
    "        if save_grad_norms:\n",
    "            np.save(f'{out_folder}/all_grad_norms_in.npy', all_grad_norms['in'])\n",
    "            np.save(f'{out_folder}/all_grad_norms_out.npy', all_grad_norms['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_checkpoint(out_folder, save_grad_norms, fit_world_only, resume):\n",
    "    \"\"\"Load checkpoint if resume is set to True and previous checkpoint exists, else create new empty checkpoint\"\"\"\n",
    "    outputs = {'out': [], 'in': []}\n",
    "    losses = {'out': [], 'in': []}\n",
    "    all_grad_norms = { 'out': [], 'in': [] }\n",
    "    train_set_accs = []\n",
    "    test_set_accs = []\n",
    "\n",
    "    if os.path.exists(out_folder) and resume:\n",
    "        # if folder exists and resume is set to true load previous values\n",
    "        random_state = dill.load(open(f'{out_folder}/random_state.dill', 'rb'))\n",
    "        np.random.set_state(random_state['np'])\n",
    "        torch.random.set_rng_state(random_state['torch'])\n",
    "\n",
    "        if fit_world_only:\n",
    "            outputs[fit_world_only] = np.load(f'{out_folder}/outputs_{fit_world_only}.npy').tolist()\n",
    "            losses[fit_world_only] = np.load(f'{out_folder}/losses_{fit_world_only}.npy').tolist()\n",
    "            if save_grad_norms:\n",
    "                all_grad_norms[fit_world_only] = np.load(f'{out_folder}/all_grad_norms_{fit_world_only}.npy').tolist()\n",
    "\n",
    "            if fit_world_only == 'out':\n",
    "                train_set_accs = np.load(f'{out_folder}/train_set_accs.npy').tolist()\n",
    "                test_set_accs = np.load(f'{out_folder}/test_set_accs.npy').tolist()\n",
    "        else:\n",
    "            outputs['in'] = np.load(f'{out_folder}/outputs_in.npy').tolist()\n",
    "            outputs['out'] = np.load(f'{out_folder}/outputs_out.npy').tolist()\n",
    "            train_set_accs = np.load(f'{out_folder}/train_set_accs.npy').tolist()\n",
    "            test_set_accs = np.load(f'{out_folder}/test_set_accs.npy').tolist()\n",
    "            losses['in'] = np.load(f'{out_folder}/losses_in.npy').tolist()\n",
    "            losses['out'] = np.load(f'{out_folder}/losses_out.npy').tolist()\n",
    "            if save_grad_norms:\n",
    "                all_grad_norms['in'] = np.load(f'{out_folder}/all_grad_norms_in.npy').tolist()\n",
    "                all_grad_norms['out'] = np.load(f'{out_folder}/all_grad_norms_out.npy').tolist()\n",
    "    else:\n",
    "        # create folder and dump initial values in\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        save_checkpoint(out_folder, outputs, losses, all_grad_norms, train_set_accs, test_set_accs, args.fit_world_only, args.save_grad_norms)\n",
    "    \n",
    "    return outputs, losses, all_grad_norms, train_set_accs, test_set_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/Users/benkim/miniconda3/envs/bb_audit_dpsgd/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "/Users/benkim/miniconda3/envs/bb_audit_dpsgd/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:67: RuntimeWarning: divide by zero encountered in log\n",
      "  z = np.log(np.where(t > np.log(1 - q), (np.exp(t) + q - 1) / q, 1))\n",
      "/Users/benkim/miniconda3/envs/bb_audit_dpsgd/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:70: RuntimeWarning: divide by zero encountered in log\n",
      "  t > np.log(1 - q),\n",
      "100%|██████████| 500/500 [27:57<00:00,  3.36s/it]\n",
      "100%|██████████| 500/500 [26:23<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses in:\n",
      "500\n",
      "[-3.6274635791778564, -3.095247745513916, -4.559689044952393, -4.911440849304199, -3.5922889709472656, -2.5989012718200684, -3.08661150932312, -2.9248905181884766, -4.156489372253418, -3.5111823081970215, -3.4447860717773438, -3.770193099975586, -4.858377456665039, -3.6240572929382324, -2.4062252044677734, -3.8240747451782227, -3.986875295639038, -3.5966930389404297, -3.323880195617676, -3.222585439682007, -3.176693916320801, -2.954719305038452, -2.8338987827301025, -3.966488838195801, -4.331681251525879, -3.2421083450317383, -2.7651009559631348, -3.527475357055664, -3.1450886726379395, -4.213202476501465, -3.1140334606170654, -3.586107015609741, -2.7853126525878906, -2.2553675174713135, -3.3396313190460205, -4.406693458557129, -4.391239643096924, -4.901767730712891, -3.1540584564208984, -3.478832960128784, -3.676720142364502, -2.922860622406006, -4.189453601837158, -2.7875044345855713, -4.94570255279541, -3.491466999053955, -4.333553791046143, -2.826115846633911, -4.045648574829102, -3.840557098388672, -3.194188356399536, -3.748347520828247, -4.083798885345459, -2.624955415725708, -5.455262184143066, -3.5594711303710938, -3.0037169456481934, -3.9336204528808594, -3.5920658111572266, -3.8600287437438965, -3.375305414199829, -4.971677303314209, -5.1989593505859375, -3.6057450771331787, -2.2137227058410645, -3.724271297454834, -3.900264263153076, -4.102312088012695, -4.41459846496582, -4.146451473236084, -3.6524648666381836, -2.141840934753418, -3.570678949356079, -2.5913052558898926, -3.7706851959228516, -3.7125635147094727, -3.373673439025879, -3.377729892730713, -3.6225242614746094, -2.7135417461395264, -1.9931312799453735, -4.296280860900879, -3.5680131912231445, -4.1092729568481445, -3.65781307220459, -3.1697134971618652, -3.8038856983184814, -3.699244976043701, -2.0249345302581787, -4.6781463623046875, -4.294131278991699, -3.8646178245544434, -2.780693292617798, -3.251598834991455, -4.068312644958496, -3.070882797241211, -4.076965808868408, -2.680027484893799, -3.9115631580352783, -4.475036144256592, -4.34909200668335, -3.2494726181030273, -4.640134334564209, -4.0981950759887695, -3.5720913410186768, -3.746471405029297, -4.5468597412109375, -4.468140125274658, -3.6738121509552, -3.9806809425354004, -5.4817795753479, -2.326934814453125, -3.104823589324951, -4.3128461837768555, -3.122602939605713, -4.087021350860596, -4.614739418029785, -4.103094100952148, -3.5427582263946533, -3.6289525032043457, -4.236381530761719, -4.686619758605957, -4.341701507568359, -3.571911096572876, -3.467298984527588, -3.078831195831299, -3.839996814727783, -2.887247085571289, -3.47829532623291, -3.4572255611419678, -3.2750401496887207, -4.017236232757568, -3.6628050804138184, -4.300022125244141, -2.415525436401367, -4.731601715087891, -2.809694290161133, -3.7281486988067627, -4.482866287231445, -5.166601181030273, -2.9510018825531006, -3.2923901081085205, -2.812220811843872, -3.316077709197998, -3.2199666500091553, -2.7798540592193604, -2.11606764793396, -3.1458580493927, -2.727447032928467, -4.206387042999268, -3.7296266555786133, -3.632040500640869, -2.907041549682617, -2.1777615547180176, -3.7859883308410645, -3.235028028488159, -4.1040191650390625, -2.5850892066955566, -3.587116003036499, -3.4168200492858887, -4.329549312591553, -3.8962323665618896, -4.185752868652344, -3.412712574005127, -2.149695873260498, -3.330684185028076, -3.1820247173309326, -2.275639533996582, -3.711604595184326, -3.270636796951294, -3.2168097496032715, -3.3673171997070312, -3.6396751403808594, -2.909038543701172, -3.1794333457946777, -3.0352072715759277, -3.582582950592041, -2.1049435138702393, -3.6123197078704834, -2.5964558124542236, -4.108153820037842, -3.8734843730926514, -3.5917086601257324, -4.11597204208374, -3.7092294692993164, -3.817296028137207, -3.2919256687164307, -2.707179069519043, -2.319920778274536, -3.2461817264556885, -4.189067363739014, -2.6280999183654785, -5.286169528961182, -4.461496353149414, -3.1820147037506104, -4.750417232513428, -2.9188294410705566, -3.9829459190368652, -4.098972797393799, -3.5739002227783203, -4.007709503173828, -3.1100943088531494, -3.029879093170166, -4.6628098487854, -2.8623580932617188, -5.63884973526001, -3.371156692504883, -3.518542766571045, -2.836024284362793, -4.5196309089660645, -2.2936275005340576, -4.006229400634766, -4.976197719573975, -3.4396398067474365, -2.8235342502593994, -4.713576316833496, -4.579170227050781, -3.3418972492218018, -4.436246871948242, -3.2047176361083984, -3.5094685554504395, -3.578312873840332, -4.459650993347168, -4.321226596832275, -3.6852166652679443, -4.086796760559082, -4.58400821685791, -3.3254075050354004, -3.550766944885254, -3.313929319381714, -5.154337406158447, -4.545012474060059, -4.603074073791504, -3.4735169410705566, -4.028866767883301, -3.175908327102661, -2.627606153488159, -2.089249610900879, -4.293381214141846, -2.845578193664551, -4.997822284698486, -3.283294200897217, -4.41301155090332, -3.7618651390075684, -5.302816390991211, -4.758077621459961, -3.9023191928863525, -2.2969884872436523, -4.685667037963867, -2.224001407623291, -4.973451137542725, -3.9863290786743164, -3.395646333694458, -4.369655609130859, -3.354599952697754, -2.94169282913208, -2.693006992340088, -3.4962940216064453, -4.091090679168701, -3.0563507080078125, -5.088667869567871, -3.8344690799713135, -3.2089426517486572, -3.143868923187256, -3.825117588043213, -3.9923737049102783, -2.638463020324707, -3.5693576335906982, -2.9727606773376465, -3.186739683151245, -3.7897160053253174, -3.4174790382385254, -3.750088691711426, -4.374918460845947, -4.833984851837158, -3.2958428859710693, -1.6382795572280884, -3.7450547218322754, -5.180844306945801, -2.3706085681915283, -3.8200900554656982, -3.9551990032196045, -2.694035053253174, -5.183732509613037, -3.7516679763793945, -3.846696615219116, -3.162055015563965, -2.6875972747802734, -2.13950777053833, -3.9280893802642822, -4.156415939331055, -3.278087615966797, -3.6597540378570557, -2.86811900138855, -3.6840806007385254, -4.012355327606201, -3.5677144527435303, -4.03139591217041, -3.043651580810547, -3.7155814170837402, -3.403416872024536, -4.126014232635498, -3.7987406253814697, -5.034102439880371, -4.513883113861084, -2.960116147994995, -4.91632080078125, -3.3460466861724854, -2.7401962280273438, -3.0799434185028076, -4.12744665145874, -3.866486072540283, -3.1407856941223145, -3.208381414413452, -4.234186172485352, -3.681607723236084, -3.158459186553955, -3.687763214111328, -3.754539728164673, -3.381793260574341, -5.2303571701049805, -4.881466865539551, -3.0052826404571533, -2.7892189025878906, -2.4816744327545166, -3.4285614490509033, -3.7230193614959717, -5.011958122253418, -2.2386841773986816, -4.071047306060791, -3.5149617195129395, -3.4938294887542725, -3.0045695304870605, -4.20332670211792, -2.588197708129883, -4.324113368988037, -5.636233329772949, -2.0288288593292236, -5.037192344665527, -4.018352508544922, -3.0723612308502197, -3.056792974472046, -2.8466639518737793, -3.1461970806121826, -2.9242002964019775, -1.6143183708190918, -2.5862324237823486, -1.9578371047973633, -5.2259111404418945, -4.180584907531738, -4.245993614196777, -4.184970378875732, -3.5571227073669434, -3.2590370178222656, -4.922302722930908, -4.61997652053833, -3.7248592376708984, -3.1148910522460938, -3.9758377075195312, -3.4379324913024902, -4.108238697052002, -3.5665740966796875, -4.068696022033691, -2.8147425651550293, -3.1164348125457764, -4.474509239196777, -3.9062860012054443, -3.840073347091675, -2.9762182235717773, -4.882292747497559, -3.496987819671631, -3.242199420928955, -3.0689427852630615, -3.1161537170410156, -3.1325764656066895, -3.2393946647644043, -3.615875244140625, -4.242685317993164, -3.3244175910949707, -3.6078805923461914, -2.9720540046691895, -4.1332316398620605, -3.754295587539673, -4.167881965637207, -3.3239288330078125, -4.61855411529541, -4.002948760986328, -3.3384687900543213, -4.080715656280518, -3.9547553062438965, -4.754771709442139, -3.539811134338379, -3.494852066040039, -3.5549068450927734, -3.4312496185302734, -3.564300775527954, -2.974658250808716, -3.605130672454834, -4.0344743728637695, -3.6551408767700195, -3.1625101566314697, -4.211249351501465, -3.591207265853882, -3.8058650493621826, -2.198253631591797, -3.624701738357544, -2.842824697494507, -2.194849729537964, -4.016445159912109, -3.3544106483459473, -3.7704458236694336, -1.772897481918335, -2.907737970352173, -3.794447898864746, -4.643097877502441, -2.0370960235595703, -5.226627349853516, -2.840890884399414, -4.092216491699219, -2.7299880981445312, -4.37637996673584, -2.8824715614318848, -3.5899600982666016, -3.395333766937256, -4.8813395500183105, -2.966635227203369, -4.877364635467529, -4.8292694091796875, -4.230422496795654, -3.0597469806671143, -4.036492824554443, -4.775637149810791, -3.0502634048461914, -3.4484658241271973, -3.6151225566864014, -4.472866058349609, -4.626284599304199, -4.652489185333252, -5.160383701324463, -3.885615348815918, -4.719135284423828, -3.705073118209839, -3.1321167945861816, -2.819760799407959, -2.5920345783233643, -3.388611316680908, -3.496262788772583, -3.53432035446167, -3.9689950942993164, -2.864254951477051, -4.887485027313232, -3.010545253753662, -3.7363362312316895, -3.546919584274292, -4.273177146911621, -3.0894694328308105, -2.9717020988464355, -4.776517391204834, -4.156267166137695, -4.444118499755859, -4.077587127685547, -3.084587574005127, -3.82157301902771, -2.318643093109131, -2.901411294937134, -2.8112356662750244, -3.8400018215179443, -3.4733378887176514, -2.716276168823242, -3.033604621887207, -2.756527900695801, -5.385142803192139, -3.560332775115967, -3.3627729415893555, -3.4278368949890137, -3.272813558578491, -4.44422721862793, -3.7824299335479736, -3.0025110244750977, -4.5588250160217285, -3.3998656272888184, -2.166119337081909, -3.809093475341797, -3.0968050956726074, -2.8003273010253906, -4.032433986663818, -5.183285236358643, -2.900296449661255, -2.9580607414245605, -3.455335855484009, -3.6462337970733643, -1.9073799848556519, -3.369530439376831, -3.9736549854278564, -3.9866907596588135, -3.473966360092163, -4.956970691680908, -4.296645641326904, -2.183530569076538, -3.9746956825256348]\n",
      "losses out:\n",
      "500\n",
      "[-3.6017117500305176, -3.7922704219818115, -5.3975701332092285, -4.359493732452393, -6.645540714263916, -5.6901936531066895, -5.550582408905029, -4.811381816864014, -4.677466869354248, -5.418756008148193, -4.900177955627441, -5.012850284576416, -4.282689094543457, -4.34400749206543, -4.282273292541504, -5.730498790740967, -6.3158392906188965, -5.906801223754883, -5.104976177215576, -4.408594131469727, -5.629857540130615, -5.861078262329102, -5.639065742492676, -5.196743965148926, -4.387765884399414, -4.201030731201172, -4.63353157043457, -3.9690210819244385, -4.943974018096924, -5.134781360626221, -5.012560844421387, -5.917489528656006, -4.500201225280762, -5.856003761291504, -4.565372467041016, -5.833869934082031, -5.6034040451049805, -5.842074871063232, -4.526865482330322, -4.472901821136475, -5.600963592529297, -4.375078201293945, -5.180659294128418, -5.321560859680176, -5.810825824737549, -4.9703192710876465, -6.055214881896973, -5.746287822723389, -4.931059837341309, -6.2446441650390625, -5.054327011108398, -6.623466968536377, -6.167826175689697, -6.16485071182251, -5.864956378936768, -4.83961296081543, -5.744346618652344, -5.716879844665527, -4.339168548583984, -5.811447620391846, -5.368132591247559, -6.106325626373291, -3.472087860107422, -4.35282564163208, -5.340266704559326, -3.7264974117279053, -5.593559741973877, -5.973742485046387, -5.036442756652832, -5.115838050842285, -5.552146911621094, -4.837165355682373, -6.28361701965332, -5.212461948394775, -6.094183444976807, -6.291637420654297, -4.959685802459717, -4.54844856262207, -5.186944961547852, -4.149898052215576, -4.693597316741943, -5.939891338348389, -5.994635105133057, -6.4706220626831055, -5.762697696685791, -5.973047256469727, -5.616729259490967, -4.685330867767334, -5.5638813972473145, -4.743739604949951, -4.121338844299316, -3.6850171089172363, -6.895864486694336, -4.745963096618652, -5.409470081329346, -4.078073501586914, -6.475883483886719, -5.507230281829834, -5.294624328613281, -4.784878253936768, -5.646721363067627, -3.5029850006103516, -5.560969829559326, -4.9993791580200195, -5.447774410247803, -4.546572685241699, -5.80933141708374, -5.268266677856445, -5.043565273284912, -4.405285835266113, -4.780155181884766, -4.625258445739746, -6.194641590118408, -5.631921291351318, -6.817999839782715, -5.609096050262451, -5.6911234855651855, -4.951061725616455, -3.466981887817383, -4.254339694976807, -4.65475606918335, -5.378094673156738, -5.818761825561523, -5.515632629394531, -5.159951686859131, -6.871933937072754, -5.580458641052246, -5.289199352264404, -4.657774925231934, -5.054941654205322, -5.869196891784668, -4.365858554840088, -3.087902545928955, -5.898063659667969, -4.850472927093506, -5.143958568572998, -4.078145980834961, -4.72166633605957, -5.813183307647705, -4.873744010925293, -4.919045448303223, -4.2017903327941895, -4.316675186157227, -6.404763698577881, -3.711517095565796, -4.821165084838867, -4.785909652709961, -4.073995113372803, -5.679932594299316, -5.212793827056885, -6.4646172523498535, -5.486389636993408, -3.9093871116638184, -6.1767578125, -4.800511360168457, -5.023152828216553, -6.011833667755127, -4.790110111236572, -4.409243106842041, -4.283679485321045, -7.077838897705078, -4.729118347167969, -5.329387664794922, -4.590752601623535, -5.393160820007324, -4.285299301147461, -4.524741172790527, -6.974919319152832, -4.740348815917969, -5.728114604949951, -4.12734317779541, -6.014641761779785, -5.888490200042725, -4.6398844718933105, -5.431303977966309, -5.412911415100098, -6.28735876083374, -5.9141526222229, -2.676896333694458, -4.93890905380249, -4.200844764709473, -4.715033531188965, -6.8383564949035645, -5.344736576080322, -5.645144462585449, -5.344015598297119, -4.855696201324463, -3.807002305984497, -5.305734634399414, -4.21125602722168, -4.866769313812256, -4.471131324768066, -5.435060501098633, -5.543705940246582, -4.8181843757629395, -4.923933506011963, -5.251038551330566, -4.812148571014404, -4.186738967895508, -7.112765312194824, -4.844844341278076, -5.18250846862793, -5.07176399230957, -6.164789199829102, -3.654153347015381, -5.575294017791748, -4.75334358215332, -3.769057273864746, -3.9247097969055176, -5.461495399475098, -5.966814041137695, -5.486983299255371, -4.352302551269531, -5.880584239959717, -2.7270877361297607, -4.209953784942627, -4.507107734680176, -4.828261375427246, -4.083677768707275, -5.438370227813721, -5.21049690246582, -5.181037425994873, -5.45937442779541, -5.217208385467529, -5.498332977294922, -6.163748741149902, -6.006476879119873, -4.851181983947754, -4.940372467041016, -4.901800155639648, -6.0952677726745605, -2.8612828254699707, -4.559604167938232, -6.006081581115723, -4.67902946472168, -5.383719444274902, -4.630891799926758, -4.323389530181885, -6.103765487670898, -5.482295513153076, -3.5686566829681396, -6.400742053985596, -4.778591156005859, -5.52205228805542, -5.124166488647461, -6.035272598266602, -5.616669654846191, -5.549274921417236, -5.139398097991943, -3.5935328006744385, -5.793686389923096, -5.386959552764893, -6.203403949737549, -6.183099746704102, -5.198768615722656, -3.951176643371582, -4.528714179992676, -5.690341472625732, -4.71221923828125, -4.805473327636719, -4.208473205566406, -5.457025527954102, -3.6815240383148193, -4.234991073608398, -5.3772664070129395, -3.701139450073242, -6.833605766296387, -4.214648246765137, -5.012724876403809, -5.537859916687012, -6.172833442687988, -3.596322536468506, -4.981767654418945, -5.199794769287109, -5.488128662109375, -5.691939353942871, -5.8322601318359375, -4.359511852264404, -4.914846420288086, -4.164621829986572, -4.655160903930664, -4.706851005554199, -6.225863456726074, -5.236841201782227, -5.392764568328857, -4.189784049987793, -4.870822429656982, -5.11625862121582, -6.932287693023682, -5.395951747894287, -6.938779830932617, -4.599651336669922, -5.118041515350342, -4.726768970489502, -4.618270397186279, -5.098133087158203, -4.895605564117432, -4.860052108764648, -4.542398452758789, -3.22615385055542, -4.244927883148193, -3.283780813217163, -4.763147354125977, -4.701513767242432, -4.823275566101074, -4.219399452209473, -4.333296775817871, -2.6643025875091553, -5.084051132202148, -4.342888355255127, -4.932521343231201, -4.670026779174805, -5.152100563049316, -4.112214088439941, -4.858957767486572, -3.660290002822876, -6.594581127166748, -4.602976322174072, -5.876594543457031, -4.589847564697266, -5.8267059326171875, -6.234427452087402, -5.36533784866333, -5.962421417236328, -6.097080707550049, -4.3461737632751465, -5.042907238006592, -3.166139602661133, -4.316268444061279, -5.382150173187256, -5.714418411254883, -4.56503438949585, -5.306075572967529, -5.751927375793457, -4.73533296585083, -4.487729072570801, -4.824556827545166, -5.285030364990234, -4.509553909301758, -4.781858444213867, -5.922874927520752, -4.5950188636779785, -3.8957114219665527, -4.542718887329102, -4.9437971115112305, -4.409198760986328, -4.280417442321777, -4.7582221031188965, -4.719158172607422, -3.1257734298706055, -3.03804087638855, -4.355715751647949, -5.1098151206970215, -6.290743827819824, -6.181428909301758, -4.19296407699585, -5.146708965301514, -5.5066752433776855, -4.47751522064209, -4.147606372833252, -6.3822808265686035, -6.708023548126221, -5.4802117347717285, -5.864037036895752, -5.144408226013184, -5.456765651702881, -4.511776447296143, -5.13214111328125, -5.368671417236328, -3.653675079345703, -4.7568039894104, -4.568267345428467, -4.234787464141846, -4.11757230758667, -4.462883472442627, -3.7728312015533447, -5.285623550415039, -4.9799065589904785, -6.375280857086182, -4.09683895111084, -4.478516578674316, -4.5673322677612305, -5.230306625366211, -6.265989780426025, -4.608364582061768, -4.563734531402588, -5.640077590942383, -3.9520277976989746, -4.1197285652160645, -6.207241058349609, -3.499147415161133, -5.589573383331299, -4.709228515625, -5.654956340789795, -5.902674674987793, -5.312069892883301, -6.08257532119751, -6.5970611572265625, -6.079051494598389, -5.869419097900391, -4.843810558319092, -3.2017273902893066, -4.340194225311279, -5.898658275604248, -5.8009867668151855, -4.902589797973633, -3.9060840606689453, -5.5579423904418945, -5.799592971801758, -4.92622709274292, -5.681349277496338, -4.4189348220825195, -5.529407501220703, -3.736720323562622, -6.399460792541504, -5.127208709716797, -5.748395919799805, -4.417280197143555, -5.138152122497559, -5.968369007110596, -4.396917819976807, -5.804433822631836, -5.16282320022583, -6.15268087387085, -4.622753143310547, -3.8663017749786377, -3.3994202613830566, -4.5179290771484375, -5.9366326332092285, -5.343552589416504, -5.901933193206787, -4.767826080322266, -5.063614845275879, -4.814792156219482, -5.389444351196289, -5.176253318786621, -5.57077169418335, -5.3744730949401855, -5.996730804443359, -5.09442138671875, -4.414808750152588, -5.225518226623535, -5.011796951293945, -3.9899086952209473, -5.254212856292725, -4.022642612457275, -4.899785041809082, -5.0911102294921875, -4.240890979766846, -5.53337287902832, -6.328827381134033, -5.126296043395996, -5.073169708251953, -5.401096820831299, -4.493229866027832, -5.264106750488281, -6.188654899597168, -4.7104315757751465, -5.318050861358643, -4.857959747314453, -6.33199405670166, -4.892740726470947, -5.387966156005859, -5.010064125061035, -6.09170389175415, -5.8099164962768555, -5.464717864990234, -6.4823455810546875, -6.078911304473877, -4.993098258972168, -3.663360357284546, -5.341322422027588, -5.750782489776611, -3.985044002532959, -4.909442901611328, -5.806772708892822, -6.555995464324951, -7.533854007720947, -5.223158836364746, -5.463621616363525, -4.331791400909424, -4.774233818054199, -3.2353708744049072, -5.178393363952637, -4.451255798339844, -4.509683609008789, -6.004858016967773, -3.5262880325317383, -5.410161972045898, -4.328310012817383, -4.636023044586182, -6.746288299560547, -4.236317157745361, -5.149796962738037, -4.742153644561768, -6.205052375793457, -5.711923599243164, -5.545065879821777, -5.025824546813965, -4.8453593254089355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]python(29689) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical eps: 6.0\n",
      "Empirical eps: 5.070635808403288\n",
      "Train set accuracy: 55.930%\n",
      "Test set accuracy: 56.076%\n"
     ]
    }
   ],
   "source": [
    "#AUDITING\n",
    "from types import SimpleNamespace\n",
    "import os, torch\n",
    "\n",
    "def _pick_device():\n",
    "    # Prefer Apple GPU (MPS) on Mac, else CUDA if present, else CPU\n",
    "    try:\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    return \"cpu\"\n",
    "\n",
    "# Use worst-case init if the notebook created it; otherwise use '' to fix to a random weight vector\n",
    "_fixed_init_path = \"pretrained_models/mnist.pt\"\n",
    "_fixed_init = _fixed_init_path if os.path.exists(_fixed_init_path) else \"\"  # '' => fix to some random weights\n",
    "# average-case Xavier init instead, set _fixed_init = None\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    data_name=\"mnist\",          # mnist | cifar10 | cifar100\n",
    "    model_name=\"lr\",            # try 'cnn' if you crafted a CNN init\n",
    "    n_reps=200,                 # total models (split across IN/OUT worlds)\n",
    "    n_df=0,                     # 0 => full dataset\n",
    "    n_epochs=100,\n",
    "    lr=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    epsilon=6.0,                # target ε (Opacus/PRV)\n",
    "    delta=1e-2,\n",
    "    target_type=\"clipbkd\",      # 'blank' | 'clipbkd' | path to a target sample\n",
    "    seed=0,\n",
    "    out=\"exp_data\",\n",
    "    device=_pick_device(),      # \"mps\" on Apple GPU, else \"cuda:0\" or \"cpu\"\n",
    "    fixed_init=_fixed_init,     # path => worst-case; '' => fixed random; None => average-case Xavier\n",
    "    block_size=1000,\n",
    "    resume=True,                # skip work if results are already present\n",
    "    fit_world_only=None,        # 'in' or 'out' to run a single world; None = both then combine\n",
    "    save_grad_norms=False,\n",
    "    alpha=0.05,                 # significance for empirical ε estimate\n",
    ")\n",
    "target = 100\n",
    "args.n_reps = 1000\n",
    "\n",
    "args.n_epochs = 5\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "out_folder = f'{args.out}/{args.data_name}_{args.model_name}_eps{args.epsilon}'\n",
    "device = args.device if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load data (define D-)\n",
    "if args.n_df == 1:\n",
    "    # load single data point for type safety\n",
    "    X_out, y_out, out_dim = load_data(args.data_name, 1, device=device)\n",
    "else:\n",
    "    X_out, y_out, out_dim = load_data(args.data_name, args.n_df - 1, device=device)\n",
    "\n",
    "init_model = None\n",
    "if args.fixed_init is not None:\n",
    "    init_model = Models[args.model_name](X_out.shape, out_dim=out_dim).to(device)\n",
    "\n",
    "    if args.fixed_init == '':\n",
    "        # initialize model (average-case)\n",
    "        xavier_init_model(init_model)\n",
    "    else:\n",
    "        # load weights from path (worst-case)\n",
    "        init_model.load_state_dict(torch.load(args.fixed_init))\n",
    "\n",
    "# craft target data point (x_T, y_T)\n",
    "if args.target_type == 'blank':\n",
    "    # blank sample\n",
    "    target_X = torch.zeros_like(X_out[[0]])\n",
    "    target_y = torch.from_numpy(np.array([9])).to(device)\n",
    "elif args.target_type == 'clipbkd':\n",
    "    # ClipBKD sample\n",
    "    target_X, target_y = craft_clipbkd(X_out, init_model, device)\n",
    "elif os.path.exists(args.target_type):\n",
    "    # pre-crafted target sample\n",
    "    target_X = torch.from_numpy(np.load(args.target_type)).to(device)\n",
    "    if init_model is not None:\n",
    "        target_y =  choose_worstcase_label(init_model, target_X)\n",
    "    else:\n",
    "        target_y = torch.from_numpy(np.array([9])).to(device)\n",
    "else:\n",
    "    raise Exception(f'Target {args.target_type} not found')\n",
    "\n",
    "# define D = D- U {(x_T, y_T)}\n",
    "X_in, y_in = torch.vstack((X_out, target_X)), torch.cat((y_out, target_y))\n",
    "\n",
    "# handle case where n_df = 1\n",
    "X_out, y_out = X_out[:args.n_df - 1], y_out[:args.n_df - 1]\n",
    "\n",
    "# load test dataset\n",
    "X_test, y_test, _ = load_data(args.data_name, None, split='test', device=device)\n",
    "\n",
    "# train M on D and D-\n",
    "# resume from checkpoint\n",
    "outputs, losses, all_grad_norms, train_set_accs, test_set_accs = resume_checkpoint(out_folder, args.save_grad_norms, args.fit_world_only, args.resume)\n",
    "worlds = [args.fit_world_only] if args.fit_world_only else ['out', 'in']\n",
    "for world in worlds:\n",
    "    # set dataset according to \"world\"\n",
    "    curr_X, curr_y = (X_out, y_out) if world == 'out' else (X_in, y_in)\n",
    "\n",
    "    # check how many reps initially completed\n",
    "    reps_completed = len(outputs[world]) \n",
    "\n",
    "    for rep in tqdm(range(reps_completed, args.n_reps // 2), initial=reps_completed, total=args.n_reps // 2):\n",
    "        # train model\n",
    "        model, grad_norms = train_model(args.model_name, curr_X, curr_y, args.epsilon, args.delta,\n",
    "            args.max_grad_norm, args.n_epochs, args.lr, device=device, init_model=init_model,\n",
    "            block_size=args.block_size, out_dim=out_dim)\n",
    "        \n",
    "        # keep track of per-sample gradient norms\n",
    "        all_grad_norms[world].append(grad_norms)\n",
    "        \n",
    "        # get loss of model on target sample\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(target_X)\n",
    "            outputs[world].append(output[0].cpu().numpy())\n",
    "            losses[world].append(-nn.CrossEntropyLoss()(output, target_y).cpu().item())\n",
    "        \n",
    "        # get test set accuracy from first 5 reps\n",
    "        if rep < 5 and world == 'out':\n",
    "            if len(X_out) > 0:\n",
    "                train_set_accs.append(test_model(model, X_out, y_out))\n",
    "            test_set_accs.append(test_model(model, X_test, y_test))\n",
    "        \n",
    "        # free CUDA memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # save checkpoint\n",
    "        save_checkpoint(out_folder, outputs, losses, all_grad_norms, train_set_accs, test_set_accs, args.fit_world_only, args.save_grad_norms)\n",
    "    outputs[world] = np.array(outputs[world])\n",
    "\n",
    "if not args.fit_world_only:\n",
    "    # calculate empirical epsilon using GDP\n",
    "    print(\"losses in:\")\n",
    "    print(np.size(losses['in']))\n",
    "    print(losses['in'])\n",
    "    print(\"losses out:\")\n",
    "    print(np.size(losses['out']))\n",
    "    print(losses['out'])\n",
    "    mia_scores = np.concatenate([losses['in'], losses['out']])\n",
    "    mia_labels = np.concatenate([np.ones_like(losses['in']), np.zeros_like(losses['out'])])\n",
    "    _, emp_eps_loss = compute_eps_lower_from_mia(mia_scores, mia_labels, args.alpha, args.delta, 'GDP', n_procs=1)\n",
    "\n",
    "    np.save(f'{out_folder}/emp_eps_loss.npy', [emp_eps_loss])\n",
    "    np.save(f'{out_folder}/mia_scores.npy', mia_scores)\n",
    "    np.save(f'{out_folder}/mia_labels.npy', mia_labels)\n",
    "\n",
    "    print(f'Theoretical eps: {args.epsilon}')\n",
    "    print(f'Empirical eps: {emp_eps_loss}')\n",
    "\n",
    "print(f'Train set accuracy: {np.mean(train_set_accs) * 100:.3f}%')\n",
    "print(f'Test set accuracy: {np.mean(test_set_accs) * 100:.3f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 0\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 250\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 500\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 750\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 1000\n",
      "Final DV Rényi estimate D_2(P||Q): 1.568266\n",
      "\n",
      "Hidden Layers: 256\n"
     ]
    }
   ],
   "source": [
    "# set the seed\n",
    "# recreate the code\n",
    "import sys\n",
    "import tensorflow as tf1\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import random as random\n",
    "\n",
    "tf1.compat.v1.disable_eager_execution()  \n",
    "tf = tf1.compat.v1 \n",
    "\n",
    "np.random.seed(0); tf.set_random_seed(0); random.seed(0)\n",
    "\n",
    "alpha     = 2      # > 1\n",
    "run_num   = 1\n",
    "n         = 1       # feature dimension\n",
    "Lrate     = 1e-5\n",
    "rescaled  = 0          # 0 or 1 (alpha-scaling in objective)\n",
    "\n",
    "\n",
    "method         = 'DV_Renyi'        \n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "mb_size = 250\n",
    "#number of nodes in each hidden layer (can have more than one hidden layer)\n",
    "hidden_layers = [256] \n",
    "\n",
    "#save estimate every SF iterations\n",
    "SF = 250\n",
    "#samples for estimating Df\n",
    "N=1000\n",
    "\n",
    "Q_pool = np.asarray(losses['in'],  dtype=np.float32).reshape(-1, 1)\n",
    "P_pool = np.asarray(losses['out'], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "#mu = (np.sum(Q_pool) + np.sum(Q_pool)) / (np.size(Q_pool)+np.size(P_pool))\n",
    "\n",
    "n = P_pool.shape[1]\n",
    "\n",
    "    \n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1.0 / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "#construct variables for the neural networks\n",
    "def initialize_W(layers):\n",
    "    W_init=[]\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W_init.append(xavier_init(size=[layers[l], layers[l+1]]))\n",
    "    return W_init\n",
    "\n",
    "def initialize_NN(layers,W_init):\n",
    "    NN_W = []\n",
    "    NN_b = []\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = tf.Variable(W_init[l])\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        NN_W.append(W)\n",
    "        NN_b.append(b)\n",
    "    return NN_W, NN_b\n",
    "\n",
    "    \n",
    "       \n",
    "alpha_scaling=1.\n",
    "if rescaled==1:\n",
    "    alpha_scaling=alpha\n",
    "\n",
    "\n",
    "\n",
    "#variable for Q\n",
    "X = tf.placeholder(tf.float32, shape=[None, n])\n",
    "#variable for P\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers=[n]+hidden_layers+ hidden_layers+ hidden_layers +[1]\n",
    "    \n",
    "\n",
    "W_init=initialize_W(layers)\n",
    "D_W, D_b = initialize_NN(layers,W_init)\n",
    "theta_D = [D_W, D_b]\n",
    "\n",
    "  \n",
    "def discriminator(x):\n",
    "    num_layers = len(D_W) + 1\n",
    "    \n",
    "    h = x\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = D_W[l]\n",
    "        b = D_b[l]\n",
    "        h = tf.nn.relu(tf.add(tf.matmul(h, W), b))\n",
    "    \n",
    "    W = D_W[-1]\n",
    "    b = D_b[-1]\n",
    "    out =  tf.matmul(h, W) + b\n",
    "\n",
    "    \n",
    "    return out      \n",
    "\n",
    "def sample_P(N_samp):\n",
    "    idx = np.random.randint(0, P_pool.shape[0], size=N_samp)\n",
    "    return P_pool[idx]  # (N_samp, n)\n",
    "\n",
    "def sample_Q(N_samp):\n",
    "    idx = np.random.randint(0, Q_pool.shape[0], size=N_samp)\n",
    "    return Q_pool[idx]\n",
    "\n",
    "P_data=discriminator(Z)\n",
    "Q_data=discriminator(X)\n",
    "\n",
    "P_max=tf.reduce_max((alpha-1.0)*P_data)\n",
    "Q_max=tf.reduce_max(alpha*Q_data)\n",
    "\n",
    "#DV renyi objective\n",
    "objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "\n",
    "#AdamOptimizer\n",
    "solver = tf.train.AdamOptimizer(learning_rate=Lrate).minimize(-objective, var_list=theta_D)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "divergence_array=np.zeros(epochs//SF+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use N samples for estimation of Df\n",
    "Q_plot_samples=sample_Q(N)\n",
    "P_plot_samples=sample_P(N)\n",
    "\n",
    "\n",
    "j=0\n",
    "for it in range(epochs+1):\n",
    "       \n",
    "    if it>0:\n",
    "\n",
    "            X_samples=sample_Q(mb_size)\n",
    "            Z_samples=sample_P(mb_size) \n",
    "\n",
    "            sess.run(solver, feed_dict={X: X_samples, Z: Z_samples})\n",
    "    \n",
    "    if it % SF == 0:                \n",
    "       \n",
    "        \n",
    "        X_samples=Q_plot_samples\n",
    "        Z_samples=P_plot_samples\n",
    "        \n",
    "        \n",
    "        divergence_array[j]=sess.run( objective, feed_dict={X: X_samples, Z: Z_samples})\n",
    "        \n",
    "        print()\n",
    "        print(method)\n",
    "        if rescaled==1:\n",
    "            print('rescaled')\n",
    "        #print('mu: {}'.format(mu))\n",
    "        print('alpha: {}'.format(alpha))\n",
    "        print('Iter: {}'.format(it))\n",
    "\n",
    "        \n",
    "        j=j+1\n",
    "        \n",
    "        \n",
    "\n",
    "print(f\"Final DV Rényi estimate D_{alpha}(P||Q): {divergence_array[-1] / alpha_scaling:.6f}\")\n",
    "\n",
    "layers_str=''\n",
    "for layer_dim in hidden_layers:\n",
    "    layers_str=layers_str+' '+str(layer_dim)\n",
    "print()\n",
    "print('Hidden Layers:'+layers_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 0\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 250\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 500\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 750\n",
      "\n",
      "DV_Renyi\n",
      "alpha: 2\n",
      "Iter: 1000\n",
      "Final DV Rényi estimate D_2(P||Q): 0.002048\n",
      "\n",
      "Hidden Layers: 256\n"
     ]
    }
   ],
   "source": [
    "# R_a (Q||P)\n",
    "\n",
    "\n",
    "# set the seed\n",
    "# recreate the code\n",
    "import sys\n",
    "import tensorflow as tf1\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import random as random\n",
    "\n",
    "tf1.compat.v1.disable_eager_execution()  \n",
    "tf = tf1.compat.v1 \n",
    "\n",
    "np.random.seed(0); tf.set_random_seed(0); random.seed(0)\n",
    "\n",
    "alpha     = 2      # > 1\n",
    "run_num   = 1\n",
    "n         = 1       # feature dimension\n",
    "Lrate     = 1e-5\n",
    "rescaled  = 0          # 0 or 1 (alpha-scaling in objective)\n",
    "mu        = 0.5        # mean shift for Q in the Gaussian example\n",
    "\n",
    "method         = 'DV_Renyi'        \n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "mb_size = 250\n",
    "#number of nodes in each hidden layer (can have more than one hidden layer)\n",
    "hidden_layers = [256] \n",
    "\n",
    "#save estimate every SF iterations\n",
    "SF = 250\n",
    "#samples for estimating Df\n",
    "N=1000\n",
    "\n",
    "Q_pool = np.asarray(losses['out'],  dtype=np.float32).reshape(-1, 1)\n",
    "P_pool = np.asarray(losses['in'], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "#mu = (np.sum(Q_pool) + np.sum(Q_pool)) / (np.size(Q_pool)+np.size(P_pool))\n",
    "\n",
    "n = P_pool.shape[1]\n",
    "\n",
    "    \n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1.0 / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "#construct variables for the neural networks\n",
    "def initialize_W(layers):\n",
    "    W_init=[]\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W_init.append(xavier_init(size=[layers[l], layers[l+1]]))\n",
    "    return W_init\n",
    "\n",
    "def initialize_NN(layers,W_init):\n",
    "    NN_W = []\n",
    "    NN_b = []\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = tf.Variable(W_init[l])\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        NN_W.append(W)\n",
    "        NN_b.append(b)\n",
    "    return NN_W, NN_b\n",
    "\n",
    "    \n",
    "       \n",
    "alpha_scaling=1.\n",
    "if rescaled==1:\n",
    "    alpha_scaling=alpha\n",
    "\n",
    "\n",
    "\n",
    "#variable for Q\n",
    "X = tf.placeholder(tf.float32, shape=[None, n])\n",
    "#variable for P\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers=[n]+hidden_layers+ hidden_layers +[1]\n",
    "    \n",
    "\n",
    "W_init=initialize_W(layers)\n",
    "D_W, D_b = initialize_NN(layers,W_init)\n",
    "theta_D = [D_W, D_b]\n",
    "\n",
    "  \n",
    "def discriminator(x):\n",
    "    num_layers = len(D_W) + 1\n",
    "    \n",
    "    h = x\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = D_W[l]\n",
    "        b = D_b[l]\n",
    "        h = tf.nn.relu(tf.add(tf.matmul(h, W), b))\n",
    "    \n",
    "    W = D_W[-1]\n",
    "    b = D_b[-1]\n",
    "    out =  tf.matmul(h, W) + b\n",
    "\n",
    "    \n",
    "    return out      \n",
    "\n",
    "def sample_P(N_samp):\n",
    "    idx = np.random.randint(0, P_pool.shape[0], size=N_samp)\n",
    "    return P_pool[idx]  # (N_samp, n)\n",
    "\n",
    "def sample_Q(N_samp):\n",
    "    idx = np.random.randint(0, Q_pool.shape[0], size=N_samp)\n",
    "    return Q_pool[idx]\n",
    "\n",
    "P_data=discriminator(Z)\n",
    "Q_data=discriminator(X)\n",
    "\n",
    "P_max=tf.reduce_max((alpha-1.0)*P_data)\n",
    "Q_max=tf.reduce_max(alpha*Q_data)\n",
    "\n",
    "#DV renyi objective\n",
    "objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "\n",
    "#AdamOptimizer\n",
    "solver = tf.train.AdamOptimizer(learning_rate=Lrate).minimize(-objective, var_list=theta_D)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "divergence_array=np.zeros(epochs//SF+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use N samples for estimation of Df\n",
    "Q_plot_samples=sample_Q(N)\n",
    "P_plot_samples=sample_P(N)\n",
    "\n",
    "\n",
    "j=0\n",
    "for it in range(epochs+1):\n",
    "       \n",
    "    if it>0:\n",
    "\n",
    "            X_samples=sample_Q(mb_size)\n",
    "            Z_samples=sample_P(mb_size) \n",
    "\n",
    "            sess.run(solver, feed_dict={X: X_samples, Z: Z_samples})\n",
    "    \n",
    "    if it % SF == 0:                \n",
    "       \n",
    "        \n",
    "        X_samples=Q_plot_samples\n",
    "        Z_samples=P_plot_samples\n",
    "        \n",
    "        \n",
    "        divergence_array[j]=sess.run( objective, feed_dict={X: X_samples, Z: Z_samples})\n",
    "        \n",
    "        print()\n",
    "        print(method)\n",
    "        if rescaled==1:\n",
    "            print('rescaled')\n",
    "        #print('mu: {}'.format(mu))\n",
    "        print('alpha: {}'.format(alpha))\n",
    "        print('Iter: {}'.format(it))\n",
    "\n",
    "        \n",
    "        j=j+1\n",
    "        \n",
    "        \n",
    "\n",
    "print(f\"Final DV Rényi estimate D_{alpha}(P||Q): {divergence_array[-1] / alpha_scaling:.6f}\")\n",
    "\n",
    "layers_str=''\n",
    "for layer_dim in hidden_layers:\n",
    "    layers_str=layers_str+' '+str(layer_dim)\n",
    "print()\n",
    "print('Hidden Layers:'+layers_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper below to create the code experiemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING PAPERS EXPERIEMNTS\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Author: Jeremiah Birrell\n",
    "\n",
    "#Neural estimation of the Renyi divergences between two n-dimensional Gaussians via 2 different methods\n",
    "#1) DV method from Variational Representations and Neural Network Estimation for Renyi Divergences,  SIAM Journal on Mathematics of Data Science}, 2021\n",
    "#2) CC method from Function-space regularized Renyi divergences, ICLR 2023\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf1\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "tf1.compat.v1.disable_eager_execution()  \n",
    "tf1.compat.v1.reset_default_graph()\n",
    "tf = tf1.compat.v1 \n",
    "\n",
    "#alpha  run_num  n  Lrate  method  IC_activation  rescaled  mu\n",
    "\n",
    "# alpha  run_num  n   Lrate  method    act   rescaled  rho\n",
    "sys.argv = ['renyi_gaussian.py', '0.5', '1', '40', '2e-4', 'DV_Renyi', 'relu', '0', '0.8']\n",
    "\n",
    "\n",
    "alpha_str =sys.argv[1]\n",
    "alpha=float(alpha_str)  # alpha>1\n",
    "run_num=int(sys.argv[2])    # run number\n",
    "n=int(sys.argv[3]) #dimension of gaussians\n",
    "Lrate=float(sys.argv[4])    # learning rate\n",
    "method=sys.argv[5] #DV_Renyi,inf_conv_Renyi\n",
    "IC_activation=sys.argv[6] #abs, elu, relu, polysoftplus\n",
    "rescaled=int(sys.argv[7]) #1 if use rescaled Renyi\n",
    "mu_str=sys.argv[8]\n",
    "mu=float(mu_str) #mean of Q distriubtion\n",
    "\n",
    "rho = float(mu_str)   \n",
    "mu  = 0.0\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "mb_size = 1000\n",
    "#number of nodes in each hidden layer (can have more than one hidden layer)\n",
    "hidden_layers = [256] \n",
    "\n",
    "\n",
    "\n",
    "#save estimate every SF iterations\n",
    "SF = 100\n",
    "#samples for estimating Df\n",
    "N=50000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#means, and covariances of the Gaussian r.v.s\n",
    "mu_p=np.zeros((n,1))\n",
    "mu_q=np.zeros((n,1))\n",
    "#mu_q[0]=mu\n",
    "\n",
    "\n",
    "\n",
    "Sigma_p=np.identity(n)\n",
    "Sigma_q=np.identity(n)\n",
    "\n",
    "\n",
    "\n",
    "#M@np.transpose(M)=Sigma\n",
    "Mp=np.linalg.cholesky(Sigma_p)\n",
    "Mq=np.linalg.cholesky(Sigma_q)\n",
    "    \n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1.0 / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "#construct variables for the neural networks\n",
    "def initialize_W(layers):\n",
    "    W_init=[]\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W_init.append(xavier_init(size=[layers[l], layers[l+1]]))\n",
    "    return W_init\n",
    "\n",
    "def initialize_NN(layers,W_init):\n",
    "    NN_W = []\n",
    "    NN_b = []\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = tf.Variable(W_init[l])\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        NN_W.append(W)\n",
    "        NN_b.append(b)\n",
    "    return NN_W, NN_b\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "Sigma_alpha=alpha*Sigma_q+(1.0-alpha)*Sigma_p       \n",
    "       \n",
    "alpha_scaling=1.\n",
    "if rescaled==1:\n",
    "    alpha_scaling=alpha\n",
    "\n",
    "\n",
    "#ADDED\n",
    "d = n // 2\n",
    "I = np.eye(d)\n",
    "Sigma_p = np.block([[I, rho*I],[rho*I, I]])  \n",
    "Sigma_q = np.eye(2*d)                        \n",
    "mu_p = np.zeros((2*d,1)); mu_q = np.zeros((2*d,1))\n",
    "\n",
    "\n",
    "#Exact R_alpha(P||Q)\n",
    "Renyi_exact=1.0/2.0*np.matmul(np.transpose(mu_q-mu_p),np.matmul(np.linalg.inv(Sigma_alpha),mu_q-mu_p))-1.0/(2.0*alpha*(alpha-1.0))*np.math.log(np.linalg.det(Sigma_alpha)/(np.math.pow(np.linalg.det(Sigma_p),1.0-alpha)*np.math.pow(np.linalg.det(Sigma_q),alpha)))\n",
    "divergence_exact=alpha_scaling*Renyi_exact[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#variable for Q\n",
    "X = tf.placeholder(tf.float32, shape=[None, n])\n",
    "#variable for P\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layers=[n]+hidden_layers+hidden_layers +[1]\n",
    "    \n",
    "\n",
    "W_init=initialize_W(layers)\n",
    "D_W, D_b = initialize_NN(layers,W_init)\n",
    "theta_D = [D_W, D_b]\n",
    "\n",
    "  \n",
    "def discriminator(x):\n",
    "    num_layers = len(D_W) + 1\n",
    "    \n",
    "    h = x\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = D_W[l]\n",
    "        b = D_b[l]\n",
    "        h = tf.nn.relu(tf.add(tf.matmul(h, W), b))\n",
    "    \n",
    "    W = D_W[-1]\n",
    "    b = D_b[-1]\n",
    "    out =  tf.matmul(h, W) + b\n",
    "\n",
    "    \n",
    "    return out        \n",
    " \n",
    "\n",
    "#IC_activation: abs, elu, relu, polysoftplus\n",
    "\n",
    "if IC_activation=='abs':\n",
    "    def IC_final_layer(y):\n",
    "        return -tf.math.abs(y)\n",
    "elif IC_activation=='elu':\n",
    "    def IC_final_layer(y):\n",
    "        return -(tf.nn.elu(y)+1.)\n",
    "elif IC_activation=='relu':\n",
    "    def IC_final_layer(y):\n",
    "        return -tf.nn.relu(y)\n",
    "elif IC_activation=='polysoftplus':\n",
    "    def IC_final_layer(y):\n",
    "        return -(1.+(1./(1.+tf.nn.relu(-y))-1)*(1.-tf.sign(y))/2. +y*(tf.sign(y)+1.)/2. )\n",
    "\n",
    "# def sample_P(N_samp):\n",
    "#     return np.transpose((mu_p+np.matmul(Mp,np.random.normal(0., 1.0, size=[n, N_samp]))))\n",
    "\n",
    "# def sample_Q(N_samp):\n",
    "#     return np.transpose((mu_q+np.matmul(Mq,np.random.normal(0., 1.0, size=[n, N_samp]))))\n",
    "\n",
    "# JOINT VS PRODUCT\n",
    "def sample_P(N_samp):\n",
    "    d = n // 2\n",
    "    X   = np.random.normal(0., 1.0, size=[N_samp, d])\n",
    "    eps = np.random.normal(0., 1.0, size=[N_samp, d])\n",
    "    Y   = rho * X + np.sqrt(1 - rho**2) * eps\n",
    "    return np.hstack([X, Y])\n",
    "\n",
    "def sample_Q(N_samp):\n",
    "    d = n // 2\n",
    "    X = np.random.normal(0., 1.0, size=[N_samp, d])\n",
    "    Y = np.random.normal(0., 1.0, size=[N_samp, d])\n",
    "    return np.hstack([X, Y])\n",
    "\n",
    "P_data=discriminator(Z)\n",
    "Q_data=discriminator(X)\n",
    "\n",
    "P_max=tf.reduce_max((alpha-1.0)*P_data)\n",
    "Q_max=tf.reduce_max(alpha*Q_data)\n",
    "\n",
    "if method=='DV_Renyi':\n",
    "    objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "elif method=='inf_conv_Renyi':\n",
    "    objective=tf.reduce_mean(IC_final_layer(Q_data))+alpha_scaling/(alpha-1.)*tf.math.log(tf.reduce_mean(tf.math.pow(-IC_final_layer(P_data)/alpha_scaling,(alpha-1.)/alpha)))+(alpha_scaling/alpha)*(np.math.log(alpha)+1.)\n",
    "\n",
    "\n",
    "\n",
    "#AdamOptimizer\n",
    "solver = tf.train.AdamOptimizer(learning_rate=Lrate).minimize(-objective, var_list=theta_D)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "divergence_array=np.zeros(epochs//SF+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use N samples for estimation of Df\n",
    "Q_plot_samples=sample_Q(N)\n",
    "P_plot_samples=sample_P(N)\n",
    "\n",
    "\n",
    "j=0\n",
    "for it in range(epochs+1):\n",
    "       \n",
    "    if it>0:\n",
    "\n",
    "            X_samples=sample_Q(mb_size)\n",
    "            Z_samples=sample_P(mb_size) \n",
    "\n",
    "            sess.run(solver, feed_dict={X: X_samples, Z: Z_samples})\n",
    "    \n",
    "    if it % SF == 0:                \n",
    "       \n",
    "        \n",
    "        X_samples=Q_plot_samples\n",
    "        Z_samples=P_plot_samples\n",
    "        \n",
    "        \n",
    "        divergence_array[j]=sess.run( objective, feed_dict={X: X_samples, Z: Z_samples})\n",
    "        \n",
    "        print()\n",
    "        print(method)\n",
    "        if rescaled==1 and not(method=='alpha_div'):\n",
    "            print('rescaled')\n",
    "        if method=='inf_conv_Renyi':\n",
    "            print(IC_activation)\n",
    "        print('mu: {}'.format(mu))\n",
    "        print('alpha: {}'.format(alpha))\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('Divergence: {:.6f}'.format(divergence_exact))\n",
    "        print('Divergence est.: {:.6f}'.format(divergence_array[j]))\n",
    "        print('Error: {:.6f}'.format(divergence_array[j]-divergence_exact))\n",
    "        print('Rel. err.: {:.6f}'.format(1.0-divergence_array[j]/divergence_exact))\n",
    "        \n",
    "        j=j+1\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "rel_err=1.-divergence_array/divergence_exact\n",
    "\n",
    "layers_str=''\n",
    "for layer_dim in hidden_layers:\n",
    "    layers_str=layers_str+' '+str(layer_dim)\n",
    "print()\n",
    "print('Hidden Layers:'+layers_str)\n",
    "\n",
    "test_name=method+'_Gaussian_est/'\n",
    "if method=='inf_conv_Renyi':\n",
    "    test_name=IC_activation+'_'+test_name\n",
    "if rescaled==1:\n",
    "    test_name='rescaled_'+test_name\n",
    "\n",
    "if not os.path.exists(test_name):\n",
    "    os.makedirs(test_name)\n",
    "    \n",
    "with open(test_name+'Exact_divergence_alpha_'+alpha_str+'_mu_'+mu_str+'_Lrate_{:.1e}'.format(Lrate)+'_epochs_'+str(epochs)+'_mbsize_'+str(mb_size)+'_dim_'+str(n)+'_layers_'+layers_str+'run'+str(run_num)+'.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow([divergence_exact])  \n",
    "\n",
    "with open(test_name+'Divergence_est_alpha_'+alpha_str+'_mu_'+mu_str+'_Lrate_{:.1e}'.format(Lrate)+'_epochs_'+str(epochs)+'_mbsize_'+str(mb_size)+'_dim_'+str(n)+'_layers_'+layers_str+'run'+str(run_num)+'.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow(divergence_array)  \n",
    "\n",
    "with open(test_name+'rel_err_alpha_'+alpha_str+'_mu_'+mu_str+'_Lrate_{:.1e}'.format(Lrate)+'_epochs_'+str(epochs)+'_mbsize_'+str(mb_size)+'_dim_'+str(n)+'_layers_'+layers_str+'run'+str(run_num)+'.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerow(rel_err)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: mean = -5.543795566082001  var = 0.054147053472015\n",
      "P: mean = -5.392570258617401  var = 0.04975853989793086\n",
      "R_10(Q || P) = 0.12328976037942072\n",
      "R_10(P || Q) = 1.1188146927223872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yb/bwm5pv5j0ms2gv7c21gj70k00000gn/T/ipykernel_97016/1941857340.py:74: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  quad = float(dmu.T @ inv_Sa @ dmu)  # scalar\n",
      "/var/folders/yb/bwm5pv5j0ms2gv7c21gj70k00000gn/T/ipykernel_97016/1941857340.py:99: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"Q: mean =\", mu_q.ravel()[0], \" var =\", float(Sigma_q))\n",
      "/var/folders/yb/bwm5pv5j0ms2gv7c21gj70k00000gn/T/ipykernel_97016/1941857340.py:100: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"P: mean =\", mu_p.ravel()[0], \" var =\", float(Sigma_p))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mle_gaussian(X):\n",
    "    \"\"\"\n",
    "    Maximum-likelihood estimates for a (uni/multi)variate Gaussian.\n",
    "\n",
    "    Args:\n",
    "        X: array-like, shape (n, d) or (n,) — samples\n",
    "\n",
    "    Returns:\n",
    "        mu:  shape (d,)                    (MLE mean)\n",
    "        Sigma: shape (d, d)                (MLE covariance, bias=True i.e., /n)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float64)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    n, d = X.shape\n",
    "    if n < 1:\n",
    "        raise ValueError(\"Need at least one sample.\")\n",
    "\n",
    "    mu = X.mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    Sigma = (Xc.T @ Xc) / n   # MLE: divide by n (not n-1)\n",
    "    return mu, Sigma\n",
    "\n",
    "\n",
    "def _logdet_psd(A, jitter=1e-12):\n",
    "    \"\"\"Stable log|A| for SPD/PSD matrices by adding a tiny jitter if needed.\"\"\"\n",
    "    A = np.asarray(A, dtype=np.float64)\n",
    "    # Symmetrize to reduce numerical asymmetry\n",
    "    A = 0.5 * (A + A.T)\n",
    "    # Try Cholesky; fall back to eig if needed\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A + jitter * np.eye(A.shape[0]))\n",
    "        return 2.0 * np.sum(np.log(np.diag(L)))\n",
    "    except np.linalg.LinAlgError:\n",
    "        w = np.linalg.eigvalsh(A + jitter * np.eye(A.shape[0]))\n",
    "        if np.any(w <= 0):\n",
    "            raise np.linalg.LinAlgError(\"Covariance not positive definite.\")\n",
    "        return np.sum(np.log(w))\n",
    "\n",
    "\n",
    "def renyi_gaussian(mu_q, Sigma_q, mu_p, Sigma_p, alpha, jitter=1e-12):\n",
    "    \"\"\"\n",
    "    Closed-form Rényi divergence R_α(Q || P) between Gaussians, matching the\n",
    "    paper's definition R_α = (1/(α(α-1))) log ∫ q^α p^{1-α}.\n",
    "\n",
    "    Args:\n",
    "        mu_q, mu_p: shape (d,)\n",
    "        Sigma_q, Sigma_p: shape (d, d)\n",
    "        alpha: float, α>0, α≠1\n",
    "        jitter: small ridge added to Σ_α for numerical stability\n",
    "\n",
    "    Returns:\n",
    "        R_alpha: float\n",
    "    \"\"\"\n",
    "    if alpha <= 0 or np.isclose(alpha, 1.0):\n",
    "        raise ValueError(\"alpha must satisfy α>0 and α≠1.\")\n",
    "\n",
    "    mu_q = np.atleast_1d(mu_q).astype(np.float64)\n",
    "    mu_p = np.atleast_1d(mu_p).astype(np.float64)\n",
    "    Sigma_q = np.atleast_2d(Sigma_q).astype(np.float64)\n",
    "    Sigma_p = np.atleast_2d(Sigma_p).astype(np.float64)\n",
    "\n",
    "    dmu = (mu_p - mu_q).reshape(-1, 1)\n",
    "    Sigma_alpha = (1.0 - alpha) * Sigma_p + alpha * Sigma_q\n",
    "\n",
    "    # Stabilize Σ_α for inversion/logdet\n",
    "    Sigma_alpha = 0.5 * (Sigma_alpha + Sigma_alpha.T) + jitter * np.eye(Sigma_alpha.shape[0])\n",
    "\n",
    "    # log ∫ q^α p^{1-α} dx  for Gaussians\n",
    "    # ln I_α = -0.5[ α(1-α) Δμᵀ Σ_α^{-1} Δμ + ln|Σ_α| - α ln|Σ_q| - (1-α) ln|Σ_p| ]\n",
    "    inv_Sa = np.linalg.inv(Sigma_alpha)\n",
    "    quad = float(dmu.T @ inv_Sa @ dmu)  # scalar\n",
    "    ln_I = -0.5 * (alpha * (1.0 - alpha) * quad\n",
    "                   + _logdet_psd(Sigma_alpha)\n",
    "                   - alpha * _logdet_psd(Sigma_q)\n",
    "                   - (1.0 - alpha) * _logdet_psd(Sigma_p))\n",
    "\n",
    "    # Paper's convention: R_α = (1/(α(α-1))) * ln I_α\n",
    "    return ln_I / (alpha * (alpha - 1.0))\n",
    "\n",
    "\n",
    "# ---- Your data → MLEs → (optional) Rényi via plug-in ------------------------\n",
    "\n",
    "# Given:\n",
    "# Q_pool = np.asarray(losses['in'],  dtype=np.float32).reshape(-1, 1)\n",
    "# P_pool = np.asarray(losses['out'], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# 1) Fit Gaussians by MLE\n",
    "mu_q, Sigma_q = mle_gaussian(P_pool)\n",
    "mu_p, Sigma_p = mle_gaussian(Q_pool)\n",
    "\n",
    "# 2) (Optional) Compute R_α(Q || P) with the Gaussian closed form\n",
    "alpha = 10  # pick your order α>0, α≠1\n",
    "R_alpha_QP = renyi_gaussian(mu_q, Sigma_q, mu_p, Sigma_p, alpha)\n",
    "R_alpha_PQ = renyi_gaussian(mu_p, Sigma_p, mu_q, Sigma_q, alpha)\n",
    "\n",
    "print(\"Q: mean =\", mu_q.ravel()[0], \" var =\", float(Sigma_q))\n",
    "print(\"P: mean =\", mu_p.ravel()[0], \" var =\", float(Sigma_p))\n",
    "print(f\"R_{alpha}(Q || P) =\", R_alpha_QP)\n",
    "print(f\"R_{alpha}(P || Q) =\", R_alpha_PQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dv_renyi_PQ():\n",
    "    tf1.compat.v1.disable_eager_execution()  \n",
    "    tf = tf1.compat.v1 \n",
    "\n",
    "    #np.random.seed(0); tf.set_random_seed(0); random.seed(0)\n",
    "\n",
    "    alpha     = 10      # > 1\n",
    "    run_num   = 1\n",
    "    n         = 1       # feature dimension\n",
    "    Lrate     = 1e-5\n",
    "    rescaled  = 0          # 0 or 1 (alpha-scaling in objective)\n",
    "\n",
    "    method         = 'DV_Renyi'        \n",
    "\n",
    "\n",
    "    epochs = 1000\n",
    "    mb_size = 250\n",
    "    #number of nodes in each hidden layer (can have more than one hidden layer)\n",
    "    hidden_layers = [256] \n",
    "\n",
    "    #save estimate every SF iterations\n",
    "    SF = 250\n",
    "    #samples for estimating Df\n",
    "    N=1000\n",
    "\n",
    "    Q_pool = np.asarray(losses['out'],  dtype=np.float32).reshape(-1, 1)\n",
    "    P_pool = np.asarray(losses['in'], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    #mu = (np.sum(Q_pool) + np.sum(Q_pool)) / (np.size(Q_pool)+np.size(P_pool))\n",
    "\n",
    "    n = P_pool.shape[1]\n",
    "\n",
    "        \n",
    "\n",
    "    def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        xavier_stddev = 1.0 / tf.sqrt(in_dim / 2.)\n",
    "        return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "    #construct variables for the neural networks\n",
    "    def initialize_W(layers):\n",
    "        W_init=[]\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W_init.append(xavier_init(size=[layers[l], layers[l+1]]))\n",
    "        return W_init\n",
    "\n",
    "    def initialize_NN(layers,W_init):\n",
    "        NN_W = []\n",
    "        NN_b = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = tf.Variable(W_init[l])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            NN_W.append(W)\n",
    "            NN_b.append(b)\n",
    "        return NN_W, NN_b\n",
    "\n",
    "        \n",
    "        \n",
    "    alpha_scaling=1.\n",
    "    if rescaled==1:\n",
    "        alpha_scaling=alpha\n",
    "\n",
    "\n",
    "\n",
    "    #variable for Q\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n])\n",
    "    #variable for P\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    layers=[n]+hidden_layers +[1]\n",
    "        \n",
    "\n",
    "    W_init=initialize_W(layers)\n",
    "    D_W, D_b = initialize_NN(layers,W_init)\n",
    "    theta_D = [D_W, D_b]\n",
    "\n",
    "    \n",
    "    def discriminator(x):\n",
    "        num_layers = len(D_W) + 1\n",
    "        \n",
    "        h = x\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = D_W[l]\n",
    "            b = D_b[l]\n",
    "            h = tf.nn.relu(tf.add(tf.matmul(h, W), b))\n",
    "        \n",
    "        W = D_W[-1]\n",
    "        b = D_b[-1]\n",
    "        out =  tf.matmul(h, W) + b\n",
    "\n",
    "        \n",
    "        return out      \n",
    "\n",
    "    def sample_P(N_samp):\n",
    "        idx = np.random.randint(0, P_pool.shape[0], size=N_samp)\n",
    "        return P_pool[idx]  # (N_samp, n)\n",
    "\n",
    "    def sample_Q(N_samp):\n",
    "        idx = np.random.randint(0, Q_pool.shape[0], size=N_samp)\n",
    "        return Q_pool[idx]\n",
    "\n",
    "    P_data=discriminator(Z)\n",
    "    Q_data=discriminator(X)\n",
    "\n",
    "    P_max=tf.reduce_max((alpha-1.0)*P_data)\n",
    "    Q_max=tf.reduce_max(alpha*Q_data)\n",
    "\n",
    "    #DV renyi objective\n",
    "    objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "    objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "\n",
    "    #AdamOptimizer\n",
    "    solver = tf.train.AdamOptimizer(learning_rate=Lrate).minimize(-objective, var_list=theta_D)\n",
    "\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "    divergence_array=np.zeros(epochs//SF+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #use N samples for estimation of Df\n",
    "    Q_plot_samples=sample_Q(N)\n",
    "    P_plot_samples=sample_P(N)\n",
    "\n",
    "\n",
    "    j=0\n",
    "    for it in range(epochs+1):\n",
    "        \n",
    "        if it>0:\n",
    "\n",
    "                X_samples=sample_Q(mb_size)\n",
    "                Z_samples=sample_P(mb_size) \n",
    "\n",
    "                sess.run(solver, feed_dict={X: X_samples, Z: Z_samples})\n",
    "        \n",
    "        if it % SF == 0:                \n",
    "        \n",
    "            \n",
    "            X_samples=Q_plot_samples\n",
    "            Z_samples=P_plot_samples\n",
    "            \n",
    "            \n",
    "            divergence_array[j]=sess.run( objective, feed_dict={X: X_samples, Z: Z_samples})\n",
    "            \n",
    "            #print()\n",
    "            #print(method)\n",
    "            if rescaled==1:\n",
    "                print('rescaled')\n",
    "            #print('mu: {}'.format(mu))\n",
    "            # print('alpha: {}'.format(alpha))\n",
    "            # print('Iter: {}'.format(it))\n",
    "\n",
    "            \n",
    "            j=j+1\n",
    "            \n",
    "            \n",
    "\n",
    "    print(f\"Final DV Rényi estimate D_{alpha}(P||Q): {divergence_array[-1] / alpha_scaling:.6f}\")\n",
    "\n",
    "    layers_str=''\n",
    "    for layer_dim in hidden_layers:\n",
    "        layers_str=layers_str+' '+str(layer_dim)\n",
    "    # print()\n",
    "    # print('Hidden Layers:'+layers_str)\n",
    "    return divergence_array[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dv_renyi_QP():\n",
    "    tf1.compat.v1.disable_eager_execution()  \n",
    "    tf = tf1.compat.v1 \n",
    "\n",
    "    #np.random.seed(0); tf.set_random_seed(0); random.seed(0)\n",
    "\n",
    "    alpha     = 10      # > 1\n",
    "    run_num   = 1\n",
    "    n         = 1       # feature dimension\n",
    "    Lrate     = 1e-5\n",
    "    rescaled  = 0          # 0 or 1 (alpha-scaling in objective)\n",
    "\n",
    "    method         = 'DV_Renyi'        \n",
    "\n",
    "\n",
    "    epochs = 1000\n",
    "    mb_size = 250\n",
    "    #number of nodes in each hidden layer (can have more than one hidden layer)\n",
    "    hidden_layers = [256] \n",
    "\n",
    "    #save estimate every SF iterations\n",
    "    SF = 250\n",
    "    #samples for estimating Df\n",
    "    N=1000\n",
    "\n",
    "    Q_pool = np.asarray(losses['out'],  dtype=np.float32).reshape(-1, 1)\n",
    "    P_pool = np.asarray(losses['in'], dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    #mu = (np.sum(Q_pool) + np.sum(Q_pool)) / (np.size(Q_pool)+np.size(P_pool))\n",
    "\n",
    "    n = P_pool.shape[1]\n",
    "\n",
    "        \n",
    "\n",
    "    def xavier_init(size):\n",
    "        in_dim = size[0]\n",
    "        xavier_stddev = 1.0 / tf.sqrt(in_dim / 2.)\n",
    "        return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "    #construct variables for the neural networks\n",
    "    def initialize_W(layers):\n",
    "        W_init=[]\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W_init.append(xavier_init(size=[layers[l], layers[l+1]]))\n",
    "        return W_init\n",
    "\n",
    "    def initialize_NN(layers,W_init):\n",
    "        NN_W = []\n",
    "        NN_b = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0,num_layers-1):\n",
    "            W = tf.Variable(W_init[l])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            NN_W.append(W)\n",
    "            NN_b.append(b)\n",
    "        return NN_W, NN_b\n",
    "\n",
    "        \n",
    "        \n",
    "    alpha_scaling=1.\n",
    "    if rescaled==1:\n",
    "        alpha_scaling=alpha\n",
    "\n",
    "\n",
    "\n",
    "    #variable for Q\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n])\n",
    "    #variable for P\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    layers=[n]+hidden_layers+[1]\n",
    "        \n",
    "\n",
    "    W_init=initialize_W(layers)\n",
    "    D_W, D_b = initialize_NN(layers,W_init)\n",
    "    theta_D = [D_W, D_b]\n",
    "\n",
    "    \n",
    "    def discriminator(x):\n",
    "        num_layers = len(D_W) + 1\n",
    "        \n",
    "        h = x\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = D_W[l]\n",
    "            b = D_b[l]\n",
    "            h = tf.nn.relu(tf.add(tf.matmul(h, W), b))\n",
    "        \n",
    "        W = D_W[-1]\n",
    "        b = D_b[-1]\n",
    "        out =  tf.matmul(h, W) + b\n",
    "\n",
    "        \n",
    "        return out      \n",
    "\n",
    "    def sample_P(N_samp):\n",
    "        idx = np.random.randint(0, P_pool.shape[0], size=N_samp)\n",
    "        return P_pool[idx]  # (N_samp, n)\n",
    "\n",
    "    def sample_Q(N_samp):\n",
    "        idx = np.random.randint(0, Q_pool.shape[0], size=N_samp)\n",
    "        return Q_pool[idx]\n",
    "\n",
    "    P_data=discriminator(Z)\n",
    "    Q_data=discriminator(X)\n",
    "\n",
    "    P_max=tf.reduce_max((alpha-1.0)*P_data)\n",
    "    Q_max=tf.reduce_max(alpha*Q_data)\n",
    "\n",
    "    #DV renyi objective\n",
    "    objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "    objective=alpha_scaling/(alpha-1.0)*tf.math.log(tf.reduce_mean(tf.math.exp(((alpha-1.0)*P_data-P_max)/alpha_scaling)))+1.0/(alpha-1.0)*P_max-1.0/alpha*Q_max-alpha_scaling/alpha*tf.math.log(tf.reduce_mean(tf.math.exp((alpha*Q_data-Q_max)/alpha_scaling)))\n",
    "\n",
    "\n",
    "    #AdamOptimizer\n",
    "    solver = tf.train.AdamOptimizer(learning_rate=Lrate).minimize(-objective, var_list=theta_D)\n",
    "\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "    divergence_array=np.zeros(epochs//SF+1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #use N samples for estimation of Df\n",
    "    Q_plot_samples=sample_Q(N)\n",
    "    P_plot_samples=sample_P(N)\n",
    "\n",
    "\n",
    "    j=0\n",
    "    for it in range(epochs+1):\n",
    "        \n",
    "        if it>0:\n",
    "\n",
    "                X_samples=sample_Q(mb_size)\n",
    "                Z_samples=sample_P(mb_size) \n",
    "\n",
    "                sess.run(solver, feed_dict={X: X_samples, Z: Z_samples})\n",
    "        \n",
    "        if it % SF == 0:                \n",
    "        \n",
    "            \n",
    "            X_samples=Q_plot_samples\n",
    "            Z_samples=P_plot_samples\n",
    "            \n",
    "            \n",
    "            divergence_array[j]=sess.run( objective, feed_dict={X: X_samples, Z: Z_samples})\n",
    "            \n",
    "            #print()\n",
    "            #print(method)\n",
    "            if rescaled==1:\n",
    "                print('rescaled')\n",
    "            #print('mu: {}'.format(mu))\n",
    "            # print('alpha: {}'.format(alpha))\n",
    "            # print('Iter: {}'.format(it))\n",
    "\n",
    "            \n",
    "            j=j+1\n",
    "            \n",
    "            \n",
    "\n",
    "    print(f\"Final DV Rényi estimate D_{alpha}(P||Q): {divergence_array[-1] / alpha_scaling:.6f}\")\n",
    "\n",
    "    layers_str=''\n",
    "    for layer_dim in hidden_layers:\n",
    "        layers_str=layers_str+' '+str(layer_dim)\n",
    "    #print()\n",
    "    #print('Hidden Layers:'+layers_str)\n",
    "    return divergence_array[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DV Rényi estimate D_10(P||Q): 0.010374\n",
      "Final DV Rényi estimate D_10(P||Q): 4.882215\n",
      "Final DV Rényi estimate D_10(P||Q): 0.582652\n",
      "Final DV Rényi estimate D_10(P||Q): -0.734646\n",
      "Final DV Rényi estimate D_10(P||Q): 2.427503\n",
      "Final DV Rényi estimate D_10(P||Q): 2.841002\n",
      "Final DV Rényi estimate D_10(P||Q): -1.501082\n",
      "Final DV Rényi estimate D_10(P||Q): 4.195619\n",
      "Final DV Rényi estimate D_10(P||Q): 1.768286\n",
      "Final DV Rényi estimate D_10(P||Q): 3.748070\n",
      "Final DV Rényi estimate D_10(P||Q): 2.405452\n",
      "Final DV Rényi estimate D_10(P||Q): 0.333911\n",
      "Final DV Rényi estimate D_10(P||Q): 4.294810\n",
      "Final DV Rényi estimate D_10(P||Q): 2.270423\n",
      "Final DV Rényi estimate D_10(P||Q): 0.165233\n",
      "Final DV Rényi estimate D_10(P||Q): -0.366689\n",
      "Final DV Rényi estimate D_10(P||Q): 1.249598\n",
      "Final DV Rényi estimate D_10(P||Q): -0.069278\n",
      "Final DV Rényi estimate D_10(P||Q): 1.913866\n",
      "Final DV Rényi estimate D_10(P||Q): 4.071554\n",
      "Final DV Rényi estimate D_10(P||Q): 4.832768\n",
      "Final DV Rényi estimate D_10(P||Q): 2.355656\n",
      "Final DV Rényi estimate D_10(P||Q): 2.059400\n",
      "Final DV Rényi estimate D_10(P||Q): 1.826342\n",
      "Final DV Rényi estimate D_10(P||Q): -0.323406\n",
      "Final DV Rényi estimate D_10(P||Q): 4.497436\n",
      "Final DV Rényi estimate D_10(P||Q): 1.151004\n",
      "Final DV Rényi estimate D_10(P||Q): 2.176462\n",
      "Final DV Rényi estimate D_10(P||Q): 2.753279\n",
      "Final DV Rényi estimate D_10(P||Q): -0.403351\n",
      "Final DV Rényi estimate D_10(P||Q): 0.341289\n",
      "Final DV Rényi estimate D_10(P||Q): 3.104676\n",
      "Final DV Rényi estimate D_10(P||Q): 0.397464\n",
      "Final DV Rényi estimate D_10(P||Q): 0.988256\n",
      "Final DV Rényi estimate D_10(P||Q): 2.520156\n",
      "Final DV Rényi estimate D_10(P||Q): 0.579261\n",
      "Final DV Rényi estimate D_10(P||Q): 1.975256\n",
      "Final DV Rényi estimate D_10(P||Q): 0.766049\n",
      "Final DV Rényi estimate D_10(P||Q): 0.525639\n",
      "Final DV Rényi estimate D_10(P||Q): 0.892706\n",
      "Dα(P||Q) mean: 1.477477\n",
      "Dα(P||Q) max : 4.832768\n",
      "Dα(P||Q) min : -1.501082\n",
      "Dα(Q||P) mean: 1.897784\n",
      "Dα(Q||P) max : 4.882215\n",
      "Dα(Q||P) min : -0.734646\n",
      "DP(P||Q) mean: 2.628770\n",
      "DP(P||Q) max : 5.984061\n",
      "DP(P||Q) min : -0.349790\n",
      "DP(Q||P) mean: 3.049076\n",
      "DP(Q||P) max : 6.033508\n",
      "DP(Q||P) min : 0.416646\n"
     ]
    }
   ],
   "source": [
    "renyi_measures_PQ = []\n",
    "renyi_measures_QP = []\n",
    "for i in range(20):\n",
    "    tf1.compat.v1.reset_default_graph()  # avoid graph accumulation\n",
    "    renyi_measures_PQ.append(dv_renyi_PQ())\n",
    "    tf1.compat.v1.reset_default_graph()  # avoid graph accumulation\n",
    "    renyi_measures_QP.append(dv_renyi_QP())\n",
    "\n",
    "renyi_measures_PQ = np.array(renyi_measures_PQ, dtype=float)\n",
    "renyi_measures_QP = np.array(renyi_measures_QP, dtype=float)\n",
    "\n",
    "print(f\"Dα(P||Q) mean: {renyi_measures_PQ.mean():.6f}\")\n",
    "print(f\"Dα(P||Q) max : {renyi_measures_PQ.max():.6f}\")\n",
    "print(f\"Dα(P||Q) min : {renyi_measures_PQ.min():.6f}\")\n",
    "\n",
    "print(f\"Dα(Q||P) mean: {renyi_measures_QP.mean():.6f}\")\n",
    "print(f\"Dα(Q||P) max : {renyi_measures_QP.max():.6f}\")\n",
    "print(f\"Dα(Q||P) min : {renyi_measures_QP.min():.6f}\")\n",
    "\n",
    "def convert(measure, alpha, delta):\n",
    "    return measure + np.log(1/delta)/(alpha - 1)\n",
    "\n",
    "print(f\"DP(P||Q) mean: {convert(renyi_measures_PQ.mean(), 5, 1e-2):.6f}\")\n",
    "print(f\"DP(P||Q) max : {convert(renyi_measures_PQ.max(), 5, 1e-2):.6f}\")\n",
    "print(f\"DP(P||Q) min : {convert(renyi_measures_PQ.min(), 5, 1e-2):.6f}\")\n",
    "\n",
    "print(f\"DP(Q||P) mean: {convert(renyi_measures_QP.mean(), 5, 1e-2):.6f}\")\n",
    "print(f\"DP(Q||P) max : {convert(renyi_measures_QP.max(), 5, 1e-2):.6f}\")\n",
    "print(f\"DP(Q||P) min : {convert(renyi_measures_QP.min(), 5, 1e-2):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP(P||Q) mean: 1.989163\n",
      "DP(P||Q) max : 5.344454\n",
      "DP(P||Q) min : -0.989397\n",
      "DP(Q||P) mean: 2.409469\n",
      "DP(Q||P) max : 5.393901\n",
      "DP(Q||P) min : -0.222961\n"
     ]
    }
   ],
   "source": [
    "print(f\"DP(P||Q) mean: {convert(renyi_measures_PQ.mean(), 10, 1e-2):.6f}\")\n",
    "print(f\"DP(P||Q) max : {convert(renyi_measures_PQ.max(), 10, 1e-2):.6f}\")\n",
    "print(f\"DP(P||Q) min : {convert(renyi_measures_PQ.min(), 10, 1e-2):.6f}\")\n",
    "\n",
    "print(f\"DP(Q||P) mean: {convert(renyi_measures_QP.mean(), 10, 1e-2):.6f}\")\n",
    "print(f\"DP(Q||P) max : {convert(renyi_measures_QP.max(), 10, 1e-2):.6f}\")\n",
    "print(f\"DP(Q||P) min : {convert(renyi_measures_QP.min(), 10, 1e-2):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.18 ('bb_audit_dpsgd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5f5d967ff3f6a283607cf7515dec61324f9620dffe005b08c45fe720d70d4ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
